{"parser": "github", "uid": "github/ropensci/robotstxt", "url": "https://github.com/ropensci/robotstxt", "data": {"timestamp": "2022-01-27 10:11:35.133996", "name": "robotstxt", "url": "https://api.github.com/repos/ropensci/robotstxt", "full_name": "ropensci/robotstxt", "html_url": "https://github.com/ropensci/robotstxt", "private": false, "description": "robots.txt file parsing and checking for R", "created_at": "2015-12-01T15:51:07Z", "updated_at": "2021-12-13T17:22:46Z", "clone_url": "https://github.com/ropensci/robotstxt.git", "homepage": "https://docs.ropensci.org/robotstxt", "size": 2700, "stargazers_count": 56, "watchers_count": 56, "language": "R", "open_issues_count": 0, "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "owner": {"html_url": "https://github.com/ropensci", "avatar_url": "https://avatars.githubusercontent.com/u/1200269?v=4", "login": "ropensci", "type": "Organization"}, "topics": ["robotstxt", "crawler", "webscraping", "spider", "scraper", "r", "rstats", "r-package", "peer-reviewed", "http-tools"]}}
