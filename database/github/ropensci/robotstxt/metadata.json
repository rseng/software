{
    "parser": "github",
    "uid": "github/ropensci/robotstxt",
    "url": "https://github.com/ropensci/robotstxt",
    "data": {
        "timestamp": "2022-01-26 19:13:46.217408",
        "name": "robotstxt",
        "url": "https://api.github.com/repos/ropensci/robotstxt",
        "full_name": "ropensci/robotstxt",
        "html_url": "https://github.com/ropensci/robotstxt",
        "private": false,
        "description": "robots.txt file parsing and checking for R",
        "created_at": "2015-12-01T15:51:07Z",
        "updated_at": "2021-12-13T17:22:46Z",
        "clone_url": "https://github.com/ropensci/robotstxt.git",
        "homepage": "https://docs.ropensci.org/robotstxt",
        "size": 2700,
        "stargazers_count": 56,
        "watchers_count": 56,
        "language": "R",
        "open_issues_count": 0,
        "license": {
            "key": "other",
            "name": "Other",
            "spdx_id": "NOASSERTION",
            "url": null,
            "node_id": "MDc6TGljZW5zZTA="
        },
        "owner": {
            "html_url": "https://github.com/ropensci",
            "avatar_url": "https://avatars.githubusercontent.com/u/1200269?v=4",
            "login": "ropensci",
            "type": "Organization"
        },
        "topics": [
            "robotstxt",
            "crawler",
            "webscraping",
            "spider",
            "scraper",
            "r",
            "rstats",
            "r-package",
            "peer-reviewed"
        ]
    }
}